{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2fc2eaf109f05a",
   "metadata": {},
   "source": [
    "# [문제 1] Fashion MNIST 데이터 정규화를 위한 Mean과 Std. 값 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T08:31:00.297086Z",
     "start_time": "2025-11-11T08:31:00.288119Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from wandb.util import download_file_into_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b11ffba521775ebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T08:35:50.155444Z",
     "start_time": "2025-11-11T08:35:40.424476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.2860\n",
      "Std: 0.3530\n"
     ]
    }
   ],
   "source": [
    "data_path = \".\"\n",
    "f_mnist_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transforms.ToTensor())\n",
    "f_mnist_train, f_mnist_validation = random_split(f_mnist_train, [55_000, 5_000])\n",
    "\n",
    "images = torch.stack([img for img, _ in f_mnist_train], dim=0)\n",
    "\n",
    "mean = images.mean()\n",
    "std = images.std()\n",
    "\n",
    "print(f\"Mean: {mean.item():.4f}\")\n",
    "print(f\"Std: {std.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f92855524e491",
   "metadata": {},
   "source": [
    "# [문제 2] Fashion MNIST 데이터에 대하여 CNN 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e833c02ed78e4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'link_dl'...\n",
      "remote: Enumerating objects: 2856, done.\u001b[K\n",
      "remote: Counting objects: 100% (351/351), done.\u001b[K\n",
      "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
      "remote: Total 2856 (delta 171), reused 302 (delta 124), pack-reused 2505 (from 2)\u001b[K\n",
      "Receiving objects: 100% (2856/2856), 60.57 MiB | 20.02 MiB/s, done.\n",
      "Resolving deltas: 100% (1940/1940), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf link_dl\n",
    "!git clone https://github.com/wjm0423/link_dl.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4c1445-da5f-4321-b0e2-63c1ae2da4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.23.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.2 MB 5.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.10.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from wandb) (21.3)\n",
      "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.26)\n",
      "Collecting typing-extensions<5,>=4.8\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 99.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (2.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.29.0)\n",
      "Collecting sentry-sdk>=2.0.0\n",
      "  Downloading sentry_sdk-2.45.0-py2.py3-none-any.whl (404 kB)\n",
      "\u001b[K     |████████████████████████████████| 404 kB 79.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.5)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (3.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.25.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 151.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging->wandb) (3.0.9)\n",
      "Installing collected packages: urllib3, typing-extensions, sentry-sdk, wandb\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dask-cudf 23.4.0 requires dask==2023.3.2, but you have dask 2022.5.1 which is incompatible.\n",
      "dask-cudf 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2022.5.1 which is incompatible.\n",
      "dask-cuda 23.4.0 requires dask==2023.3.2, but you have dask 2022.5.1 which is incompatible.\n",
      "dask-cuda 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2022.5.1 which is incompatible.\n",
      "cuml 23.4.0 requires dask==2023.3.2, but you have dask 2022.5.1 which is incompatible.\n",
      "cuml 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2022.5.1 which is incompatible.\n",
      "cugraph 23.4.0 requires dask==2023.3.2, but you have dask 2022.5.1 which is incompatible.\n",
      "cugraph 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2022.5.1 which is incompatible.\n",
      "cugraph-service-server 23.4.0 requires dask==2023.3.2, but you have dask 2022.5.1 which is incompatible.\n",
      "cugraph-service-server 23.4.0 requires distributed==2023.3.2.1, but you have distributed 2022.5.1 which is incompatible.\n",
      "botocore 1.12.253 requires urllib3<1.26,>=1.20; python_version >= \"3.4\", but you have urllib3 1.26.20 which is incompatible.\u001b[0m\n",
      "Successfully installed sentry-sdk-2.45.0 typing-extensions-4.15.0 urllib3-1.26.20 wandb-0.23.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4caf64d-df79-4814-a8b5-59768b150831",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/work/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwjm0423\u001b[0m (\u001b[33mwjm0423-korea-university-of-technology-and-education\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6aa9495-a293-487e-90b6-5b18dff4d32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0a0f1cc-a831-4443-a8cf-89fedc1ab292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁█</td></tr><tr><td>Training accuracy (%)</td><td>▁█</td></tr><tr><td>Training loss</td><td>█▁</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▁█</td></tr><tr><td>Validation accuracy (%)</td><td>▁█</td></tr><tr><td>Validation loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>10</td></tr><tr><td>Training accuracy (%)</td><td>97.64727</td></tr><tr><td>Training loss</td><td>0.07063</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.08475</td></tr><tr><td>Validation accuracy (%)</td><td>91.82</td></tr><tr><td>Validation loss</td><td>0.2597</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet_2025-11-22_08-35-49</strong> at: <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/56x1v2uw' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/56x1v2uw</a><br> View project at: <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251122_083549-56x1v2uw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/wandb/run-20251122_083940-vf7m9sar</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/vf7m9sar' target=\"_blank\">resnet_2025-11-22_08-39-40</a></strong> to <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/vf7m9sar' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/vf7m9sar</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "namespace(wandb=True, batch_size=512, epochs=200, learning_rate=0.01, validation_intervals=10, early_stop_patience=7, early_stop_delta=1e-05, weight_decay=0.0005)\n",
      "{'epochs': 200, 'batch_size': 512, 'validation_intervals': 10, 'learning_rate': 0.01, 'early_stop_patience': 7, 'early_stop_delta': 1e-05, 'weight_decay': 0.0005}\n",
      "Training on device cuda:0.\n",
      "Num Train Samples:  55000\n",
      "Num Validation Samples:  5000\n",
      "Sample Data Shape:  torch.Size([1, 28, 28])\n",
      "Sample Data Target:  1\n",
      "Number of Data Loading Workers: 3\n",
      "[Epoch   1] T_loss: 0.57706, T_accuracy: 79.2036 | V_loss: 0.31476, V_accuracy: 88.5600 | Early stopping is stated! | T_time: 00:00:12, T_speed: 0.083\n",
      "[Epoch  10] T_loss: 0.02572, T_accuracy: 99.1382 | V_loss: 0.33584, V_accuracy: 91.8800 | Early stopping counter: 1 out of 7 | T_time: 00:01:58, T_speed: 0.085\n",
      "[Epoch  20] T_loss: 0.00034, T_accuracy: 100.0000 | V_loss: 0.29797, V_accuracy: 93.5400 | V_loss decreased (0.31476 --> 0.29797). Saving model... | T_time: 00:03:56, T_speed: 0.085\n",
      "[Epoch  30] T_loss: 0.00024, T_accuracy: 100.0000 | V_loss: 0.28896, V_accuracy: 93.6400 | V_loss decreased (0.29797 --> 0.28896). Saving model... | T_time: 00:05:53, T_speed: 0.085\n",
      "[Epoch  40] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28686, V_accuracy: 93.6600 | V_loss decreased (0.28896 --> 0.28686). Saving model... | T_time: 00:07:51, T_speed: 0.085\n",
      "[Epoch  50] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28788, V_accuracy: 93.6200 | Early stopping counter: 1 out of 7 | T_time: 00:09:49, T_speed: 0.085\n",
      "[Epoch  60] T_loss: 0.00024, T_accuracy: 100.0000 | V_loss: 0.28510, V_accuracy: 93.7800 | V_loss decreased (0.28686 --> 0.28510). Saving model... | T_time: 00:11:46, T_speed: 0.085\n",
      "[Epoch  70] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28418, V_accuracy: 93.7400 | V_loss decreased (0.28510 --> 0.28418). Saving model... | T_time: 00:13:43, T_speed: 0.085\n",
      "[Epoch  80] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28490, V_accuracy: 93.6600 | Early stopping counter: 1 out of 7 | T_time: 00:15:41, T_speed: 0.085\n",
      "[Epoch  90] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28548, V_accuracy: 93.6800 | Early stopping counter: 2 out of 7 | T_time: 00:17:38, T_speed: 0.085\n",
      "[Epoch 100] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28657, V_accuracy: 93.7400 | Early stopping counter: 3 out of 7 | T_time: 00:19:36, T_speed: 0.085\n",
      "[Epoch 110] T_loss: 0.00023, T_accuracy: 100.0000 | V_loss: 0.28560, V_accuracy: 93.7000 | Early stopping counter: 4 out of 7 | T_time: 00:21:34, T_speed: 0.085\n",
      "[Epoch 120] T_loss: 0.00024, T_accuracy: 100.0000 | V_loss: 0.28566, V_accuracy: 93.7400 | Early stopping counter: 5 out of 7 | T_time: 00:23:32, T_speed: 0.085\n",
      "[Epoch 130] T_loss: 0.00024, T_accuracy: 100.0000 | V_loss: 0.28612, V_accuracy: 93.6600 | Early stopping counter: 6 out of 7 | T_time: 00:25:29, T_speed: 0.085\n",
      "[Epoch 140] T_loss: 0.00024, T_accuracy: 100.0000 | V_loss: 0.28649, V_accuracy: 93.6800 | Early stopping counter: 7 out of 7 *** TRAIN EARLY STOPPED! *** | T_time: 00:27:28, T_speed: 0.085\n",
      "Final training time: 00:27:28\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇█</td></tr><tr><td>Training accuracy (%)</td><td>▁██████████████</td></tr><tr><td>Training loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Training speed (epochs/sec.)</td><td>▁▇▇█▇▇█████████</td></tr><tr><td>Validation accuracy (%)</td><td>▁▅█████████████</td></tr><tr><td>Validation loss</td><td>▅█▃▂▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>140</td></tr><tr><td>Training accuracy (%)</td><td>100</td></tr><tr><td>Training loss</td><td>0.00024</td></tr><tr><td>Training speed (epochs/sec.)</td><td>0.08495</td></tr><tr><td>Validation accuracy (%)</td><td>93.68</td></tr><tr><td>Validation loss</td><td>0.28649</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet_2025-11-22_08-39-40</strong> at: <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/vf7m9sar' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist/runs/vf7m9sar</a><br> View project at: <a href='https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist' target=\"_blank\">https://wandb.ai/wjm0423-korea-university-of-technology-and-education/cnn_fashion_mnist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251122_083940-vf7m9sar/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "try:\n",
    "  BASE_PATH = str(Path(__file__).resolve().parent.parent.parent)  # BASE_PATH: /Users/yhhan/git/link_dl\n",
    "except NameError:\n",
    "  BASE_PATH = str(Path.cwd() / 'link_dl')\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "os.listdir(BASE_PATH)\n",
    "\n",
    "try:\n",
    "  CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "  CURRENT_FILE_PATH = str(Path.cwd())\n",
    "CHECKPOINT_FILE_PATH = os.path.join(CURRENT_FILE_PATH, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINT_FILE_PATH, exist_ok=True)\n",
    "if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "  os.makedirs(CHECKPOINT_FILE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from _01_code._09_fcn_best_practice.c_trainer import ClassificationTrainer\n",
    "from _03_homeworks.homework_3.a_fashion_mnist_data import get_fashion_mnist_data\n",
    "from _01_code._16_modern_cnns.a_arg_parser import get_parser\n",
    "\n",
    "\n",
    "def get_resnet_model(num_classes=10):\n",
    "    class ResnetBlock(nn.Module):\n",
    "\n",
    "      def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------------------------------\n",
    "        # ResNet 블록: H(x) = x + F(x)의 F(x)\n",
    "        # ------------------------------------\n",
    "        # ResNet 블록의 첫 번째 Convolution Layer\n",
    "        # 입력 피처 맵(in_channels)을 받아 out_channels개의 3 × 3 필터를 학습하여 출력 피처 맵을 생성\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        # 배치 정규화: 각 피처에 대해 배치 단위로 평균과 분산을 계산해 정규화\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 활성화 함수-ReLU: 입력값 x가 0 이하면 0, 0 초과면 x 반환\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # ResNet 블록의 첫 번째 Convolution Layer 이후 Convolution Layer\n",
    "        # 블록 내부에서 필터링만 수행\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # H(x) = x + F(x)에서 x와 F(x)의 크기가 다를 때 x(=identity)에 적용하는 다운샘플링\n",
    "        # stride가 1이 아닐 때 크기 차이가 발생하기 때문에 stride != 1 or self.in_channels != out_channels로 조건을 준다.\n",
    "        self.downsample = downsample\n",
    "\n",
    "      # ResNet 블록의 순전파 순서\n",
    "      def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # stride != 1 or self.in_channels != out_channels이면 identity에 다운샘플링을 적용하여 out과 크기를 맞춤.\n",
    "        if self.downsample is not None:\n",
    "          identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------\n",
    "    # ResNet-18\n",
    "    # ------------------------------------\n",
    "    class ResNet18(nn.Module):\n",
    "      def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 처음 stem 부분 → Conv 사용\n",
    "        # 입력: Fashion-MNIST 이미지 → 1 × 28 × 28\n",
    "        # Fashion-MNIST 이미지는 흑백 이미지이기 때문에 RGB값이 없어 채널 값은 1\n",
    "        # B × 1 × 28 × 28 --> B × 64 × {(28 - 3 + 2) / 1 + 1} × {(28 - 3 + 2) / 1 + 1} = B × 64 × 28 × 28\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # ResNet stages (2 blocks × 4 layers): 각 layer는 ResNetBlock을 2개씩 쌓은 것.\n",
    "        # B × 64 × 28 × 28 --> B × 64 × {(28 - 3 + 2) / 1 + 1} × {(28 - 3 + 2) / 1 + 1} = B × 64 × 28 × 28\n",
    "        self.layer1 = self._make_layer(out_channels=64, blocks=2, stride=1)\n",
    "        # B × 64 × 28 ×28 --> B × 128 × {(28 - 3 + 2) / 2 + 1} × {(28 - 3 + 2) / 2 + 1} = B × 128 × 14 × 14\n",
    "        self.layer2 = self._make_layer(out_channels=128, blocks=2, stride=2)\n",
    "        # B × 128 × 28 ×28 --> B × 256 × {(14 - 3 + 2) / 2 + 1} × {(14 - 3 + 2) / 2 + 1} = B × 256 × 7 × 7\n",
    "        self.layer3 = self._make_layer(out_channels=256, blocks=2, stride=2)\n",
    "        # B × 256 × 7 ×7 --> B × 512 × {(7 - 3 + 2) / 2 + 1} × {(7 - 3 + 2) / 2 + 1} = B × 512 × 4 × 4\n",
    "        self.layer4 = self._make_layer(out_channels=512, blocks=2, stride=2)\n",
    "\n",
    "        # B × 512 × 4 × 4 --> B × 512 × 1 × 1\n",
    "        # 각 채널에 대해 모든 4 × 4 공간 차원의 평균값을 계산하여 1 × 1 크기의 텐서로 만듦. 즉, 각 샘플을 512개의 특징 벡터로 요약.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.4) # p: dropout probability\n",
    "        # B × 512 --> B × 10\n",
    "        # 512개의 특징 벡터를 가지고 판단하여 샘플이 10개(num_classes)의 클래스별로 분류될 확률을 계산\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "      def _make_layer(self, out_channels, blocks, stride):\n",
    "        # ResNet 블록들을 쌓아 하나의 layer를 생성\n",
    "        downsample = None\n",
    "\n",
    "        # 필요한 경우 identity 다운샘플링\n",
    "        # downsample 로직 → Conv/BatchNorm 활용\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "          downsample = nn.Sequential(\n",
    "              nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "              nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # 첫 번째 ResNet 블록을 만들어 layers에 추가\n",
    "        layers = []\n",
    "        layers.append(ResnetBlock(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        # 나머지 ResNet 블록을 만들어 layers에 추가하여 반환\n",
    "        for _ in range(1, blocks):\n",
    "          layers.append(ResnetBlock(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "      def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        \"\"\"\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # B × 512 × 1 × 1 --> B × 512\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # ResNet18 인스턴스를 만들어 반환\n",
    "    my_model = ResNet18()\n",
    "\n",
    "    return my_model\n",
    "\n",
    "\n",
    "def main(args):\n",
    "  # 프로젝트 이름이나 체크포인트에 사용하기 위해 현재 시간을 타임스탬프로 생성\n",
    "  run_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "  # 학습 설정\n",
    "  config = {\n",
    "      'epochs': args.epochs,\n",
    "      'batch_size': args.batch_size,\n",
    "      'validation_intervals': args.validation_intervals,\n",
    "      'learning_rate': args.learning_rate,\n",
    "      'early_stop_patience': args.early_stop_patience,\n",
    "      'early_stop_delta': args.early_stop_delta,\n",
    "      'weight_decay': args.weight_decay\n",
    "  }\n",
    "\n",
    "  # Wandb에 프로젝트 로깅하여 출력\n",
    "  project_name = \"cnn_fashion_mnist\"\n",
    "  name = \"resnet_{0}\".format(run_time_str)\n",
    "  wandb.init(\n",
    "      mode=\"online\" if args.wandb else \"disabled\",\n",
    "      project=project_name,\n",
    "      notes=\"fashion mnist experiment with resnet\",\n",
    "      tags=[\"resnet\", \"fashion_mnist\"],\n",
    "      name=name,\n",
    "      config=config\n",
    "  )\n",
    "  print(args)\n",
    "  print(wandb.config)\n",
    "\n",
    "  # CUDA 가능 여부에 따라 GPU나 CPU 선택\n",
    "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "  print(f\"Training on device {device}.\")\n",
    "\n",
    "  # 학습용, 검증용 DataLoader 및 Data Augmentation을 적용한 이미지 transform을 호출\n",
    "  train_data_loader, validation_data_loader, fashion_mnist_transforms = get_fashion_mnist_data()\n",
    "  model = get_resnet_model(num_classes=10)\n",
    "  model.to(device)\n",
    "\n",
    "  # ResNet18 모델 생성 후 GPU/CPU 디바이스로 이동\n",
    "  from torchinfo import summary\n",
    "  summary(model=model,\n",
    "          input_size=(1, 1, 28, 28),\n",
    "          col_names=[\"kernel_size\", \"input_size\", \"output_size\", \"num_params\", \"mult_adds\"]\n",
    "  )\n",
    "\n",
    "  # 옵티마이저로 SGD를 사용\n",
    "  optimizer = optim.SGD(\n",
    "      model.parameters(),\n",
    "      lr=wandb.config.learning_rate,\n",
    "      momentum=0.9,\n",
    "      weight_decay=config['weight_decay']\n",
    "  )\n",
    "\n",
    "  # 학습률 스케줄러\n",
    "  scheduler = lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "  # Fashion-MNIST 데이터셋 분류 훈련용 객체 생성.\n",
    "  # train_loop()로 훈련, 검증, early stopping, 체크포인트 저장, wandb 로깅 등을 수행\n",
    "  classification_trainer = ClassificationTrainer(\n",
    "      project_name, model, optimizer, train_data_loader, validation_data_loader, fashion_mnist_transforms,\n",
    "      run_time_str, wandb, device, CHECKPOINT_FILE_PATH, scheduler=scheduler\n",
    "  )\n",
    "  classification_trainer.train_loop()\n",
    "\n",
    "  # wandb 세션 종료\n",
    "  wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  # parser = get_parser()\n",
    "  # args = parser.parse_args()\n",
    "  # python _01_code/_09_modern_cnns/_02_googlenet/a_cifar10_train_googlenet.py --wandb -v 10\n",
    "  from types import SimpleNamespace\n",
    "\n",
    "  args = SimpleNamespace(\n",
    "      wandb=True,\n",
    "      batch_size=512,\n",
    "      epochs=200,\n",
    "      learning_rate=0.01,\n",
    "      validation_intervals=10,\n",
    "      early_stop_patience=7,\n",
    "      early_stop_delta=1e-05,\n",
    "      weight_decay=0.0005\n",
    "  )\n",
    "\n",
    "  main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd7ae7b7664b04",
   "metadata": {},
   "source": [
    "# [문제 3] 학습 완료된 모델로 테스트 데이터 Accuracy\t확인하기\n",
    "# [문제 4] 샘플 테스트 데이터 분류 예측 결과 확인하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583286d6cdc9e678",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/link_dl\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 26421880/26421880 [00:03<00:00, 6758032.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/train-images-idx3-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 29515/29515 [00:00<00:00, 103970.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 4422102/4422102 [00:02<00:00, 1751795.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 5148/5148 [00:00<00:00, 6896287.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /home/work/link_dl/_00_data/j_fashion_mnist/FashionMNIST/raw\n",
      "\n",
      "Num Test Samples:  10000\n",
      "Sample Shape:  torch.Size([1, 28, 28])\n",
      "MODEL FILE: /home/work/checkpoints/cnn_fashion_mnist_checkpoint_latest.pt\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from torch import nn, optim\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    BASE_PATH = str(Path(__file__).resolve().parent.parent.parent)  # BASE_PATH: /Users/yhhan/git/link_dl\n",
    "except NameError:\n",
    "    BASE_PATH = str(Path.cwd() / 'link_dl')\n",
    "try:\n",
    "    CURRENT_FILE_PATH = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    CURRENT_FILE_PATH = str(Path.cwd())\n",
    "CHECKPOINT_FILE_PATH = os.path.join(CURRENT_FILE_PATH, \"checkpoints\")\n",
    "os.makedirs(CHECKPOINT_FILE_PATH, exist_ok=True)\n",
    "if not os.path.isdir(CHECKPOINT_FILE_PATH):\n",
    "  os.makedirs(CHECKPOINT_FILE_PATH)\n",
    "\n",
    "import sys\n",
    "sys.path.append(BASE_PATH)\n",
    "\n",
    "from _01_code._09_fcn_best_practice.d_tester import ClassificationTester\n",
    "from _03_homeworks.homework_3.a_fashion_mnist_data import get_fashion_mnist_test_data\n",
    "\n",
    "import torchvision\n",
    "\n",
    "USE_PYTORCH_MODEL = False\n",
    "\n",
    "def get_resnet_model(num_classes=10):\n",
    "    class ResnetBlock(nn.Module):\n",
    "\n",
    "      def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # ------------------------------------\n",
    "        # ResNet 블록: H(x) = x + F(x)의 F(x)\n",
    "        # ------------------------------------\n",
    "        # ResNet 블록의 첫 번째 Convolution Layer\n",
    "        # 입력 피처 맵(in_channels)을 받아 out_channels개의 3 × 3 필터를 학습하여 출력 피처 맵을 생성\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "        )\n",
    "        # 배치 정규화: 각 피처에 대해 배치 단위로 평균과 분산을 계산해 정규화\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 활성화 함수-ReLU: 입력값 x가 0 이하면 0, 0 초과면 x 반환\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # ResNet 블록의 첫 번째 Convolution Layer 이후 Convolution Layer\n",
    "        # 블록 내부에서 필터링만 수행\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # H(x) = x + F(x)에서 x와 F(x)의 크기가 다를 때 x(=identity)에 적용하는 다운샘플링\n",
    "        # stride가 1이 아닐 때 크기 차이가 발생하기 때문에 stride != 1 or self.in_channels != out_channels로 조건을 준다.\n",
    "        self.downsample = downsample\n",
    "\n",
    "      # ResNet 블록의 순전파 순서\n",
    "      def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # stride != 1 or self.in_channels != out_channels이면 identity에 다운샘플링을 적용하여 out과 크기를 맞춤.\n",
    "        if self.downsample is not None:\n",
    "          identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    # ------------------------------------\n",
    "    # ResNet-18\n",
    "    # ------------------------------------\n",
    "    class ResNet18(nn.Module):\n",
    "      def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 처음 stem 부분 → Conv 사용\n",
    "        # 입력: Fashion-MNIST 이미지 → 1 × 28 × 28\n",
    "        # Fashion-MNIST 이미지는 흑백 이미지이기 때문에 RGB값이 없어 채널 값은 1\n",
    "        # B × 1 × 28 × 28 --> B × 64 × {(28 - 3 + 2) / 1 + 1} × {(28 - 3 + 2) / 1 + 1} = B × 64 × 28 × 28\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1, bias=False),\n",
    "            nn.BatchNorm2d(num_features=64, eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # ResNet stages (2 blocks × 4 layers): 각 layer는 ResNetBlock을 2개씩 쌓은 것.\n",
    "        # B × 64 × 28 × 28 --> B × 64 × {(28 - 3 + 2) / 1 + 1} × {(28 - 3 + 2) / 1 + 1} = B × 64 × 28 × 28\n",
    "        self.layer1 = self._make_layer(out_channels=64, blocks=2, stride=1)\n",
    "        # B × 64 × 28 ×28 --> B × 128 × {(28 - 3 + 2) / 2 + 1} × {(28 - 3 + 2) / 2 + 1} = B × 128 × 14 × 14\n",
    "        self.layer2 = self._make_layer(out_channels=128, blocks=2, stride=2)\n",
    "        # B × 128 × 28 ×28 --> B × 256 × {(14 - 3 + 2) / 2 + 1} × {(14 - 3 + 2) / 2 + 1} = B × 256 × 7 × 7\n",
    "        self.layer3 = self._make_layer(out_channels=256, blocks=2, stride=2)\n",
    "        # B × 256 × 7 ×7 --> B × 512 × {(7 - 3 + 2) / 2 + 1} × {(7 - 3 + 2) / 2 + 1} = B × 512 × 4 × 4\n",
    "        self.layer4 = self._make_layer(out_channels=512, blocks=2, stride=2)\n",
    "\n",
    "        # B × 512 × 4 × 4 --> B × 512 × 1 × 1\n",
    "        # 각 채널에 대해 모든 4 × 4 공간 차원의 평균값을 계산하여 1 × 1 크기의 텐서로 만듦. 즉, 각 샘플을 512개의 특징 벡터로 요약.\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout(p=0.4) # p: dropout probability\n",
    "        # B × 512 --> B × 10\n",
    "        # 512개의 특징 벡터를 가지고 판단하여 샘플이 10개(num_classes)의 클래스별로 분류될 확률을 계산\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "      def _make_layer(self, out_channels, blocks, stride):\n",
    "        # ResNet 블록들을 쌓아 하나의 layer를 생성\n",
    "        downsample = None\n",
    "\n",
    "        # 필요한 경우 identity 다운샘플링\n",
    "        # downsample 로직 → Conv/BatchNorm 활용\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "          downsample = nn.Sequential(\n",
    "              nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "              nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "        # 첫 번째 ResNet 블록을 만들어 layers에 추가\n",
    "        layers = []\n",
    "        layers.append(ResnetBlock(self.in_channels, out_channels, stride=stride, downsample=downsample))\n",
    "\n",
    "        self.in_channels = out_channels\n",
    "\n",
    "        # 나머지 ResNet 블록을 만들어 layers에 추가하여 반환\n",
    "        for _ in range(1, blocks):\n",
    "          layers.append(ResnetBlock(out_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "      def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the net.\n",
    "        \"\"\"\n",
    "        x = self.stem(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        # B × 512 × 1 × 1 --> B × 512\n",
    "        x = self.dropout(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # ResNet18 인스턴스를 만들어 반환\n",
    "    my_model = ResNet18()\n",
    "\n",
    "    return my_model\n",
    "\n",
    "def main():\n",
    "    f_mnist_test_images, test_data_loader, f_mnist_transforms = get_fashion_mnist_test_data()\n",
    "    \n",
    "    # 테스트 모델 로드\n",
    "    test_model = torchvision.models.resnet18(num_classes=10) if USE_PYTORCH_MODEL else get_resnet_model(num_classes=10)\n",
    "    \n",
    "    # 분류 테스터 객체 생성 및 전체 정확도 평가\n",
    "    project_name = \"cnn_fashion_mnist\"\n",
    "    classification_tester = ClassificationTester(\n",
    "        project_name, test_model, test_data_loader, f_mnist_transforms, CHECKPOINT_FILE_PATH\n",
    "    )\n",
    "    # 훈련 완료된 모델로 테스트 데이터의 정확도 확인\n",
    "    classification_tester.test()\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # 샘플 테스트 데이터 10개 샘플의 분류 예측 결과 확인\n",
    "    # -----------------------------------------------------\n",
    "    num_test_samples = len(f_mnist_test_images)\n",
    "    sample_indices = random.sample(range(num_test_samples), 10)\n",
    "    misclassified_found = False\n",
    "    \n",
    "    # Fashion MNIST 클래스 레이블 매핑\n",
    "    labels_map = {\n",
    "        0: \"T-shirt/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\", \n",
    "        5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle boot\"\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"                 [문제 4] 10개 샘플 예측 결과\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        img_tensor, label_int = f_mnist_test_images[idx]\n",
    "        \n",
    "        # 모델 예측\n",
    "        predicted_int = classification_tester.test_single(img_tensor)\n",
    "        \n",
    "        # 결과 비교\n",
    "        is_correct = (predicted_int == label_int)\n",
    "        \n",
    "        print(f\"\\n--- 샘플 {i+1} (Index: {idx}) ---\")\n",
    "        print(f\"정답 레이블 (ID): {label_int} ({labels_map[label_int]})\")\n",
    "        print(f\"예측 결과 (ID): {predicted_int} ({labels_map[predicted_int]})\")\n",
    "        print(f\"일치 여부: {'O (정답)' if is_correct else 'X (오분류)'}\")\n",
    "        \n",
    "        # 오분류된 샘플의 이미지 출력 및 해석 준비\n",
    "        if not is_correct:\n",
    "            misclassified_found = True\n",
    "            \n",
    "            # 이미지 출력\n",
    "            plt.figure(figsize=(2, 2))\n",
    "            plt.imshow(img_tensor.squeeze().numpy(), cmap='gray')\n",
    "            plt.title(f\"Label: {labels_map[label_int]}, Pred: {labels_map[predicted_int]}\")\n",
    "            plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba0a67f",
   "metadata": {},
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaed3b",
   "metadata": {},
   "source": [
    "시험해봐야 할 하이퍼 파라미터 조합이 많은데 Backend.ai, 구글 코랩 모두 GPU로 돌려도 훈련을 한 번 시행할 때마다 최소 30분은 걸려서 시간을 매우 많이 잡아먹었습니다.\n",
    "\n",
    "또한 Data Augmentation은 과제에 나와있는대로 f_mnist_transforms 객체에 넣어서 구현하는 게 불가능해서 과제 수행 중간에 많은 시행착오를 겪어야 했습니다. 다음 과제 때는 실행 시간을 줄이는 디폴트 하이퍼 파라미터 값이 주어지면 좋겠습니다.\n",
    "\n",
    "심지어 마지막 테스트 데이터 실행 때는 Backend.ai의 VRAM 부족 오류로 하지도 못해 매우 안타깝게 생각합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
